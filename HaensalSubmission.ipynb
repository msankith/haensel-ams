{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('sample.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frameValues=dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=dataframe.ix[:,0:294]\n",
    "Y=dataframe.ix[:,295]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if any values are null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Checking if same value is present in all data points. We can delete these columns, since it doesnt not add any value for prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "179\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n"
     ]
    }
   ],
   "source": [
    "\n",
    "temp = np.transpose(frameValues.as_matrix())\n",
    "itr=0\n",
    "columnsWithSameValue =[]\n",
    "for data in temp:\n",
    "    if data[3] == data[7]:\n",
    "        columnsWithSameValue.append(itr)\n",
    "    itr+=1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Build covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/numpy/lib/function_base.py:3003: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/usr/local/lib/python3.4/dist-packages/numpy/lib/function_base.py:3004: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "features = X.shape[1]\n",
    "correfOutput= np.ndarray(shape=[features,features])\n",
    "for i in range(features):\n",
    "    for j in range(features):\n",
    "        res=np.corrcoef(X[i],X[j])\n",
    "        correfOutput[i][j]=res[0][1]\n",
    "        correfOutput[j][i]=res[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresCovariance={}\n",
    "for i in range(features):\n",
    "    for j in range(i+1,features):\n",
    "        if abs(correfOutput[i][j])>0.7:\n",
    "            if i in featuresCovariance:\n",
    "                featuresCovariance[i].append(j)\n",
    "            else:\n",
    "                featuresCovariance[i]=[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following set of columns are highly correlated- Some times its better to remove highly correlated features.\n",
    "In order to remove the highly correlated features- have used Chi2 metrics to decide which column to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: [44], 6: [45], 64: [294], 78: [285], 177: [257]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresCovariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/sparse/compressed.py:130: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  if np.rank(self.data) != 1 or np.rank(self.indices) != 1 or np.rank(self.indptr) != 1:\n",
      "/usr/lib/python3/dist-packages/scipy/sparse/coo.py:200: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  if np.rank(self.data) != 1 or np.rank(self.row) != 1 or np.rank(self.col) != 1:\n"
     ]
    }
   ],
   "source": [
    "canBeRemoved = []\n",
    "for key in featuresCovariance:\n",
    "    feaImportanceScore = chi2(X.ix[:,key:key+1],Y)[0][0]\n",
    "    toBeRemoved=key\n",
    "    for fea2 in  featuresCovariance[key]:\n",
    "        temp = chi2(X.ix[:,fea2:fea2+1],Y)[0][0]\n",
    "        if temp < feaImportanceScore:\n",
    "            feaImportanceScore= temp\n",
    "            toBeRemoved=fea2\n",
    "    canBeRemoved.append(toBeRemoved)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#removing the columns/features\n",
    "X.drop(columnsWithSameValue,inplace=True,axis=1)\n",
    "X.drop(canBeRemoved,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Normalizing the features, using min max scaler and selecting top 100 features/columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(X)\n",
    "df_normalized = pd.DataFrame(np_scaled)\n",
    "X=df_normalized\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "kbest = SelectKBest(f_classif,k=100)\n",
    "X = kbest.fit_transform(X2,Y)\n",
    "X=pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into training and test/validation dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66137\n",
      "(49602, 100) (49602,)\n",
      "(16535, 100) (16535,)\n"
     ]
    }
   ],
   "source": [
    "labels= pd.factorize(Y)[0]\n",
    "\n",
    "splitRatio = int(len(X)*0.75)\n",
    "train_features = X.ix[:splitRatio-1]\n",
    "train_labels = labels[:splitRatio]\n",
    "test_features = X.ix[splitRatio:]\n",
    "test_labels = labels[splitRatio:]\n",
    "print(len(X))\n",
    "print(train_features.shape,train_labels.shape)\n",
    "print(test_features.shape,test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign weights to loss function function - since we have imbalance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14029968096526907,\n",
       " 0.03790616447676792,\n",
       " 0.7088619078579312,\n",
       " 0.013109152214342955,\n",
       " 0.0998230944856888]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_features\n",
    "import collections\n",
    "val=collections.Counter(Y)\n",
    "weights =[]\n",
    "for e in val:\n",
    "    weights.append(float(val[e])/len(Y))\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network code \n",
    "\n",
    "Have used 2 layer perceptron : with first hidden units of 60 and second layer of 15\n",
    "Last layer using Softmax classifier\n",
    "Used weighted cross entropy loss as loss function\n",
    "Adam optimizer for optimizing the loss function\n",
    "\n",
    "Used F1 score as performance measure - because of imbalance dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputData = tf.placeholder(tf.float32,[None,X.shape[1]])\n",
    "target = tf.placeholder(tf.float32,[None,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))\n",
    "\n",
    "\n",
    "def neuralNetwork(inputData,inputSize,outputSize,isSoftmax=False):\n",
    "    \n",
    "    weights = new_weights([inputSize,outputSize])\n",
    "    biases = new_biases(outputSize)\n",
    "    \n",
    "    layer = tf.add(tf.matmul(inputData, weights), biases)\n",
    "    layer = tf.nn.relu (layer)\n",
    "    if isSoftmax:\n",
    "        return tf.nn.softmax(layer)\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hiddenUnit1=60\n",
    "hiddenUnit2=15\n",
    "classes=5\n",
    "learning_rate = 0.00001\n",
    "layer_1 = neuralNetwork(inputData,inputSize=train_features.shape[1],outputSize=hiddenUnit1)\n",
    "layer_2 = neuralNetwork(layer_1,inputSize=hiddenUnit1,outputSize=hiddenUnit2)\n",
    "outputLayer = neuralNetwork(layer_2,inputSize=hiddenUnit2,outputSize=classes,isSoftmax=True)\n",
    "\n",
    "\n",
    "error = tf.nn.softmax_cross_entropy_with_logits(logits=outputLayer, labels=target)\n",
    "# print(error)\n",
    "# scaled_error = tf.matmul(error, [weights])\n",
    "# cost = tf.reduce_mean(scaled_error)\n",
    "# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=outputLayer, labels=target))\n",
    "cost =tf.reduce_mean(-tf.reduce_sum(target*tf.log(outputLayer) + (1-target)*tf.log(1-outputLayer), reduction_indices=1))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(outputLayer, 1), tf.argmax(target, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-be7c5c03aee9>:2: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Iter 0, Minibatch Loss= 2.558726, Training Accuracy= 0.00000\n",
      "Iter 10000, Minibatch Loss= 2.502012, Training Accuracy= 0.00000\n",
      "Iter 20000, Minibatch Loss= 2.502012, Training Accuracy= 0.10000\n",
      "Iter 30000, Minibatch Loss= 2.502012, Training Accuracy= 0.10000\n",
      "Iter 40000, Minibatch Loss= 2.502012, Training Accuracy= 0.10000\n",
      "Epoch  0, Minibatch Loss= 2.501832, Training Accuracy= 0.10056\n",
      "Iter 49610, Minibatch Loss= 2.502012, Training Accuracy= 0.10000\n",
      "Iter 59610, Minibatch Loss= 2.502012, Training Accuracy= 0.00000\n",
      "Iter 69610, Minibatch Loss= 2.502012, Training Accuracy= 0.10000\n",
      "Iter 79610, Minibatch Loss= 2.065642, Training Accuracy= 0.60000\n",
      "Iter 89610, Minibatch Loss= 1.593470, Training Accuracy= 0.80000\n",
      "Epoch  1, Minibatch Loss= 1.668677, Training Accuracy= 0.70473\n",
      "Iter 99220, Minibatch Loss= 1.851181, Training Accuracy= 0.50000\n",
      "Iter 109220, Minibatch Loss= 1.016908, Training Accuracy= 0.80000\n",
      "Iter 119220, Minibatch Loss= 1.739530, Training Accuracy= 0.60000\n",
      "Iter 129220, Minibatch Loss= 1.945935, Training Accuracy= 0.60000\n",
      "Iter 139220, Minibatch Loss= 1.512181, Training Accuracy= 0.80000\n",
      "Epoch  2, Minibatch Loss= 1.657845, Training Accuracy= 0.70473\n",
      "Iter 148830, Minibatch Loss= 1.870173, Training Accuracy= 0.50000\n",
      "Iter 158830, Minibatch Loss= 1.013979, Training Accuracy= 0.80000\n",
      "Iter 168830, Minibatch Loss= 1.743916, Training Accuracy= 0.60000\n",
      "Iter 178830, Minibatch Loss= 1.937631, Training Accuracy= 0.60000\n",
      "Iter 188830, Minibatch Loss= 1.495628, Training Accuracy= 0.80000\n",
      "Epoch  3, Minibatch Loss= 1.654454, Training Accuracy= 0.70473\n",
      "Iter 198440, Minibatch Loss= 1.876244, Training Accuracy= 0.50000\n",
      "Iter 208440, Minibatch Loss= 1.013471, Training Accuracy= 0.80000\n",
      "Iter 218440, Minibatch Loss= 1.745162, Training Accuracy= 0.60000\n",
      "Iter 228440, Minibatch Loss= 1.937075, Training Accuracy= 0.60000\n",
      "Iter 238440, Minibatch Loss= 1.487245, Training Accuracy= 0.80000\n",
      "Epoch  4, Minibatch Loss= 1.651437, Training Accuracy= 0.70473\n",
      "Iter 248050, Minibatch Loss= 1.879661, Training Accuracy= 0.50000\n",
      "Iter 258050, Minibatch Loss= 1.013150, Training Accuracy= 0.80000\n",
      "Iter 268050, Minibatch Loss= 1.745995, Training Accuracy= 0.60000\n",
      "Iter 278050, Minibatch Loss= 1.937659, Training Accuracy= 0.60000\n",
      "Iter 288050, Minibatch Loss= 1.480029, Training Accuracy= 0.80000\n",
      "Epoch  5, Minibatch Loss= 1.648525, Training Accuracy= 0.70473\n",
      "Iter 297660, Minibatch Loss= 1.882643, Training Accuracy= 0.50000\n",
      "Iter 307660, Minibatch Loss= 1.012833, Training Accuracy= 0.80000\n",
      "Iter 317660, Minibatch Loss= 1.746866, Training Accuracy= 0.60000\n",
      "Iter 327660, Minibatch Loss= 1.938495, Training Accuracy= 0.60000\n",
      "Iter 337660, Minibatch Loss= 1.473166, Training Accuracy= 0.80000\n",
      "Epoch  6, Minibatch Loss= 1.645679, Training Accuracy= 0.70473\n",
      "Iter 347270, Minibatch Loss= 1.885519, Training Accuracy= 0.50000\n",
      "Iter 357270, Minibatch Loss= 1.012554, Training Accuracy= 0.80000\n",
      "Iter 367270, Minibatch Loss= 1.747775, Training Accuracy= 0.60000\n",
      "Iter 377270, Minibatch Loss= 1.939348, Training Accuracy= 0.60000\n",
      "Iter 387270, Minibatch Loss= 1.466732, Training Accuracy= 0.80000\n",
      "Epoch  7, Minibatch Loss= 1.642877, Training Accuracy= 0.70473\n",
      "Iter 396880, Minibatch Loss= 1.888095, Training Accuracy= 0.50000\n",
      "Iter 406880, Minibatch Loss= 1.012376, Training Accuracy= 0.80000\n",
      "Iter 416880, Minibatch Loss= 1.749074, Training Accuracy= 0.60000\n",
      "Iter 426880, Minibatch Loss= 1.940364, Training Accuracy= 0.60000\n",
      "Iter 436880, Minibatch Loss= 1.460759, Training Accuracy= 0.80000\n",
      "Epoch  8, Minibatch Loss= 1.640178, Training Accuracy= 0.70473\n",
      "Iter 446490, Minibatch Loss= 1.890235, Training Accuracy= 0.50000\n",
      "Iter 456490, Minibatch Loss= 1.012205, Training Accuracy= 0.80000\n",
      "Iter 466490, Minibatch Loss= 1.750061, Training Accuracy= 0.60000\n",
      "Iter 476490, Minibatch Loss= 1.941312, Training Accuracy= 0.60000\n",
      "Iter 486490, Minibatch Loss= 1.454807, Training Accuracy= 0.80000\n",
      "Epoch  9, Minibatch Loss= 1.637524, Training Accuracy= 0.70473\n"
     ]
    }
   ],
   "source": [
    "sess= tf.Session() \n",
    "init_op = tf.initialize_all_variables()\n",
    "sess.run(init_op)\n",
    "\n",
    "batchSize=10\n",
    "display_step = 10000\n",
    "step=0\n",
    "epoch=0\n",
    "maxEpoch = 10\n",
    "while epoch < maxEpoch:\n",
    "    itr=0;\n",
    "    while itr < len(train_features):\n",
    "        batch_Feature = train_features.ix[itr:itr+batchSize-1]\n",
    "        batch_Labels = np.eye(classes)[train_labels[itr:itr+batchSize]]\n",
    "        sess.run(optimizer, feed_dict={inputData: batch_Feature, target: batch_Labels})\n",
    "\n",
    "        if itr % display_step == 0:\n",
    "                loss, acc,err = sess.run([cost, accuracy,outputLayer], feed_dict={inputData: batch_Feature,\n",
    "                                                                  target: batch_Labels})\n",
    "                print (\"Iter \" + str(step*batchSize) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "        itr+=batchSize\n",
    "        step+=1\n",
    "    batch_Labels = np.eye(classes)[train_labels]\n",
    "    loss, acc = sess.run([cost, accuracy], feed_dict={inputData: train_features,\n",
    "                                                                  target:batch_Labels })\n",
    "    print (\"Epoch  \" + str(epoch) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "    epoch+=1\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_Labels = np.eye(classes)[test_labels]\n",
    "pred = sess.run(outputLayer,feed_dict={inputData: test_features,target:batch_Labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictedLabels=np.argmax(pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72125793770789237"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_labels,predictedLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.721257937708\n",
      "0.167611819683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(test_labels,predictedLabels,average='micro'))\n",
    "print(f1_score(test_labels,predictedLabels,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
